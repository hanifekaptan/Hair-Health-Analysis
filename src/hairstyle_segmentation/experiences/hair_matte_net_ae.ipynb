{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2587a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa6ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greenBlock(inputTensor, channel, strides):\n",
    "    \"\"\"\n",
    "    Burada basemodel üzerinden kullanılacak katmanlar şunlardır:\n",
    "    112*112*64: block_1_expand_relu (112*112*96)\n",
    "    56*56*128: block_3_expand_relu (56*56*144)\n",
    "    28*28*256: block_6_expand_relu (28*28*192)\n",
    "    14*14*512: block_13_expand_relu (14*14*576)\n",
    "    7*7*1024: out_relu (7*7*1280)\n",
    "    \"\"\"\n",
    "    tensor = keras.layers.DepthwiseConv2D(\n",
    "        kernel_size = 3,\n",
    "        strides = 1,\n",
    "        padding = \"same\",\n",
    "        use_bias = False\n",
    "    )(inputTensor)\n",
    "    tensor = keras.layers.BatchNormalization()(tensor)\n",
    "    tensor = keras.layers.ReLU()(tensor)\n",
    "    tensor = keras.layers.Conv2D(\n",
    "        filters = channel,\n",
    "        kernel_size = 1,\n",
    "        padding = \"same\",\n",
    "        use_bias = False\n",
    "    )(tensor)\n",
    "    tensor = keras.layers.BatchNormalization()(tensor)\n",
    "    skipConnection = keras.layers.ReLU()(tensor)\n",
    "    tensor = keras.layers.MaxPooling2D(strides)(skipConnection)\n",
    "    return tensor, skipConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f8e7386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orangeBlock(inputTensor, channel):\n",
    "    tensor = keras.layers.DepthwiseConv2D(\n",
    "        kernel_size=3,\n",
    "        padding=\"same\"\n",
    "    )(inputTensor)\n",
    "    tensor = keras.layers.Conv2D(\n",
    "        filters = channel,\n",
    "        kernel_size = 1,\n",
    "        padding=\"same\"\n",
    "    )(tensor)\n",
    "    relu = keras.layers.ReLU()(tensor)\n",
    "    return relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f92e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redBlock(inputTensor):\n",
    "    tensor = keras.layers.Conv2D(\n",
    "        filters = 2,\n",
    "        kernel_size = 1,\n",
    "        padding=\"same\"\n",
    "    )(inputTensor)\n",
    "    tensor = keras.layers.Softmax()(tensor)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "276e4d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blueBlock(inputTensor):\n",
    "    # flatten katmanı ekleyelim\n",
    "    # tensor = keras.layers.Flatten()(inputTensor)\n",
    "    tensor = keras.layers.Conv2D(\n",
    "        filters = 32,\n",
    "        strides = 2,\n",
    "        kernel_size = 3,\n",
    "        padding=\"same\"\n",
    "    )(inputTensor)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a314a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yellowBlock(inputLayer1, inputLayer2, channel):\n",
    "    layer1 = keras.layers.Conv2D(\n",
    "        filters = channel,\n",
    "        kernel_size = 1,\n",
    "        padding = \"same\",\n",
    "        activation = \"relu\"\n",
    "    )(inputLayer1)\n",
    "    layer2 = keras.layers.UpSampling2D(\n",
    "        size = 2,\n",
    "        interpolation = \"bilinear\"\n",
    "    )(inputLayer2)\n",
    "    tensor = keras.layers.Add()([layer1, layer2])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c5f8f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(input_boyutu=(224, 224, 3)):\n",
    "    \"\"\"\n",
    "    keras.applications.MobileNetV2 omurgası kullanarak derin öğrenme modeli oluşturur (Encoder-Decoder mimarisi).\n",
    "\n",
    "    Bu model, girdi görüntüsünden bir maske (segmentasyon maskesi) üretmek için tasarlanmıştır.\n",
    "    Encoder bölümü olarak önceden eğitilmiş keras.applications.MobileNetV2 modeli kullanılır.\n",
    "    Decoder bölümü ise yukarı örnekleme ve basit konvolüsyon blokları ile maskeyi oluşturur.\n",
    "    Atlama bağlantıları (skip connections) encoder\"dan decoder\"a detay bilgisi taşır.\n",
    "    \"\"\"\n",
    "    baseModel = keras.applications.MobileNetV2(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=input_boyutu\n",
    "    )\n",
    "    baseModel.trainable = False\n",
    "\n",
    "    # MobileNetV2'den alınacak katmanların isimleri ve çıktıları\n",
    "    # layer_names = [\n",
    "    #     'block_1_expand_relu',   # ~112x112\n",
    "    #     'block_3_expand_relu',   # ~56x56\n",
    "    #     'block_6_expand_relu',   # ~28x28\n",
    "    #     'block_13_expand_relu'   # ~14x14\n",
    "    # ]\n",
    "    # mobileNetSublayers = {name: baseModel.get_layer(name).output for name in layer_names}\n",
    "\n",
    "    # Encoder çıkışını al (en derin katman)\n",
    "    # inputName = 'out_relu'\n",
    "    # tensor = baseModel.get_layer(inputName).output # 7x7x1280\n",
    "\n",
    "    # ENCODER\n",
    "    inputTensor = baseModel.input\n",
    "    tensor = blueBlock(inputTensor) # 112*112*32\n",
    "    print(tensor.shape)\n",
    "    tensor, skipConnection112 = greenBlock(tensor, 64, 1) # 112*112*64\n",
    "    print(tensor.shape)\n",
    "    tensor, _ = greenBlock(tensor, 128, 2) # 56*56*128\n",
    "    print(tensor.shape)\n",
    "    tensor, skipConnection56 = greenBlock(tensor, 128, 1) # 56*56*128\n",
    "    print(tensor.shape)\n",
    "    tensor, _ = greenBlock(tensor, 256, 2) # 28*28*256\n",
    "    print(tensor.shape)\n",
    "    tensor, skipConnection28 = greenBlock(tensor, 256, 1) # 28*28*256\n",
    "    print(tensor.shape)\n",
    "    tensor, _ = greenBlock(tensor, 512, 2) # 14*14*512\n",
    "    print(tensor.shape)\n",
    "    tensor, _ = greenBlock(tensor, 512, 1) # 14*14*512  \n",
    "    print(tensor.shape)\n",
    "    # tensor, _ = greenBlock(tensor, 512, 1) # 14*14*512\n",
    "    # print(tensor.shape)\n",
    "    # tensor, _ = greenBlock(tensor, 512, 1) # 14*14*512\n",
    "    # print(tensor.shape)\n",
    "    tensor, _ = greenBlock(tensor, 512, 1) # 14*14*512\n",
    "    print(tensor.shape)\n",
    "    tensor, skipConnection14 = greenBlock(tensor, 512, 1) # 14*14*512\n",
    "    print(tensor.shape)\n",
    "    tensor, _ = greenBlock(tensor, 1024, 2) # 7*7*1024\n",
    "    print(tensor.shape)\n",
    "    tensor, _ = greenBlock(tensor, 1024, 1) # 7*7*1024\n",
    "    print(tensor.shape)\n",
    "    print(\"--------------------------\")\n",
    "    \n",
    "    # DECODER\n",
    "    tensor = yellowBlock(skipConnection14, tensor, 1024) # 14*14*1024\n",
    "    print(tensor.shape)\n",
    "    tensor = orangeBlock(tensor, 64) # 14*14*64\n",
    "    print(tensor.shape)\n",
    "    tensor = yellowBlock(skipConnection28, tensor, 64) # 28*28*64\n",
    "    print(tensor.shape)\n",
    "    tensor = orangeBlock(tensor, 64) # 28*28*64\n",
    "    print(tensor.shape)\n",
    "    tensor = yellowBlock(skipConnection56, tensor, 64) # 56*56*64\n",
    "    print(tensor.shape)\n",
    "    tensor = orangeBlock(tensor, 64) # 56*56*64\n",
    "    print(tensor.shape)\n",
    "    tensor = yellowBlock(skipConnection112, tensor, 64) # 112*112*64\n",
    "    print(tensor.shape)\n",
    "    tensor = orangeBlock(tensor, 64) # 112*112*64\n",
    "    print(tensor.shape)\n",
    "    tensor = keras.layers.UpSampling2D(\n",
    "        size = 2,\n",
    "        interpolation = \"bilinear\"\n",
    "    )(tensor) # 224*224*64\n",
    "    print(tensor.shape)\n",
    "    tensor = orangeBlock(tensor, 64) # 224*224*64\n",
    "    print(tensor.shape)\n",
    "    tensor = redBlock(tensor) # 224*224*2\n",
    "    print(tensor.shape)\n",
    "    return keras.Model(inputs=baseModel.input, outputs=tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee59645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resizeDataset(imageDirectory, maskDirectory, targetSize=(224, 224), batchSize=8):\n",
    "    resizeAndRescale = keras.Sequential([\n",
    "        keras.layers.Resizing(targetSize[0], targetSize[1]),\n",
    "        keras.layers.Rescaling(1./255)\n",
    "    ])\n",
    "\n",
    "    imageDataset = keras.utils.image_dataset_from_directory(\n",
    "        imageDirectory,\n",
    "        batch_size=batchSize,\n",
    "        seed=42,\n",
    "        image_size=targetSize,\n",
    "        label_mode = None,\n",
    "        color_mode = \"rgb\",\n",
    "        shuffle = False\n",
    "    )\n",
    "\n",
    "    maskDataset = keras.utils.image_dataset_from_directory(\n",
    "        maskDirectory,\n",
    "        batch_size=batchSize,\n",
    "        seed=42,\n",
    "        image_size=targetSize,\n",
    "        label_mode = None,\n",
    "        color_mode = \"grayscale\",\n",
    "        shuffle = False\n",
    "    )\n",
    "\n",
    "    imageDataset = imageDataset.map(lambda x: resizeAndRescale(x))\n",
    "    maskDataset = maskDataset.map(lambda x: resizeAndRescale(x))\n",
    "    # Maskeleri normalleştir ve uygun formata dönüştür\n",
    "    def convert_to_binary_masks(mask):        \n",
    "        # 0.5 eşik değeri ile ikili maske oluştur\n",
    "        mask = tf.cast(mask > 0.5, tf.float32)\n",
    "        \n",
    "        # Her piksel için [arka plan, saç] kanallarını oluştur\n",
    "        background = 1 - mask\n",
    "        hair = mask\n",
    "        \n",
    "        # İki kanalı birleştir: [arka plan, saç]\n",
    "        one_hot_mask = tf.concat([background, hair], axis=-1)\n",
    "        return one_hot_mask\n",
    "    \n",
    "    maskDataset = maskDataset.map(convert_to_binary_masks)\n",
    "\n",
    "    # Veri setlerini birleştir\n",
    "    dataset = tf.data.Dataset.zip((imageDataset, maskDataset))\n",
    "    dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "438bfca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 112, 112, 32)\n",
      "(None, 112, 112, 64)\n",
      "(None, 56, 56, 128)\n",
      "(None, 56, 56, 128)\n",
      "(None, 28, 28, 256)\n",
      "(None, 28, 28, 256)\n",
      "(None, 14, 14, 512)\n",
      "(None, 14, 14, 512)\n",
      "(None, 14, 14, 512)\n",
      "(None, 14, 14, 512)\n",
      "(None, 7, 7, 1024)\n",
      "(None, 7, 7, 1024)\n",
      "--------------------------\n",
      "(None, 14, 14, 1024)\n",
      "(None, 14, 14, 64)\n",
      "(None, 28, 28, 64)\n",
      "(None, 28, 28, 64)\n",
      "(None, 56, 56, 64)\n",
      "(None, 56, 56, 64)\n",
      "(None, 112, 112, 64)\n",
      "(None, 112, 112, 64)\n",
      "(None, 224, 224, 64)\n",
      "(None, 224, 224, 64)\n",
      "(None, 224, 224, 2)\n"
     ]
    }
   ],
   "source": [
    "model = createModel()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adadelta(learning_rate=1.0, rho=0.95, epsilon=1e-7),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\n",
    "        \"accuracy\", \"precision\", \"recall\",\n",
    "        keras.metrics.MeanIoU(num_classes=2),\n",
    "        keras.metrics.IoU(num_classes=2, target_class_ids=[1]),\n",
    "        \"categorical_accuracy\", \n",
    "        \"mse\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cf045f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63ad3c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=r\"E:\\Ben\\TRYazilim\\hair_analysis_test_project\\src\\hair_type_segmentation\\deneme.keras\",\n",
    "        monitor = \"val_loss\",\n",
    "        save_best_only = True,\n",
    "        mode = \"min\",\n",
    "        verbose = 1,\n",
    "        save_weights_only = False\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bccdeb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 840 files.\n",
      "Found 0 files.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No images found in directory E:\\Ben\\TRYazilim\\Figaro-1k\\train\\mask. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainDataset \u001b[38;5;241m=\u001b[39m resizeDataset(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBen\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTRYazilim\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFigaro-1k\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBen\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTRYazilim\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFigaro-1k\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m testDataset \u001b[38;5;241m=\u001b[39m resizeDataset(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBen\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTRYazilim\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFigaro-1k\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBen\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTRYazilim\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFigaro-1k\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m, in \u001b[0;36mresizeDataset\u001b[1;34m(imageDirectory, maskDirectory, targetSize, batchSize)\u001b[0m\n\u001b[0;32m      2\u001b[0m resizeAndRescale \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m      3\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mResizing(targetSize[\u001b[38;5;241m0\u001b[39m], targetSize[\u001b[38;5;241m1\u001b[39m]),\n\u001b[0;32m      4\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mRescaling(\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m      5\u001b[0m ])\n\u001b[0;32m      7\u001b[0m imageDataset \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[0;32m      8\u001b[0m     imageDirectory,\n\u001b[0;32m      9\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatchSize,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m maskDataset \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[0;32m     18\u001b[0m     maskDirectory,\n\u001b[0;32m     19\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatchSize,\n\u001b[0;32m     20\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m     21\u001b[0m     image_size\u001b[38;5;241m=\u001b[39mtargetSize,\n\u001b[0;32m     22\u001b[0m     label_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     23\u001b[0m     color_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrayscale\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m     shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m imageDataset \u001b[38;5;241m=\u001b[39m imageDataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: resizeAndRescale(x))\n\u001b[0;32m     28\u001b[0m maskDataset \u001b[38;5;241m=\u001b[39m maskDataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: resizeAndRescale(x))\n",
      "File \u001b[1;32mc:\\Users\\kaptan\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\image_dataset_utils.py:329\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[0;32m    325\u001b[0m image_paths, labels \u001b[38;5;241m=\u001b[39m dataset_utils\u001b[38;5;241m.\u001b[39mget_training_or_validation_split(\n\u001b[0;32m    326\u001b[0m     image_paths, labels, validation_split, subset\n\u001b[0;32m    327\u001b[0m )\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m image_paths:\n\u001b[1;32m--> 329\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images found in directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAllowed formats: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mALLOWLIST_FORMATS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    332\u001b[0m     )\n\u001b[0;32m    334\u001b[0m dataset \u001b[38;5;241m=\u001b[39m paths_and_labels_to_dataset(\n\u001b[0;32m    335\u001b[0m     image_paths\u001b[38;5;241m=\u001b[39mimage_paths,\n\u001b[0;32m    336\u001b[0m     image_size\u001b[38;5;241m=\u001b[39mimage_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    347\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m    348\u001b[0m )\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: No images found in directory E:\\Ben\\TRYazilim\\Figaro-1k\\train\\mask. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')"
     ]
    }
   ],
   "source": [
    "trainDataset = resizeDataset(r\"E:\\Ben\\TRYazilim\\Figaro-1k\\train\\image\", r\"E:\\Ben\\TRYazilim\\Figaro-1k\\train\\mask\")\n",
    "testDataset = resizeDataset(r\"E:\\Ben\\TRYazilim\\Figaro-1k\\test\\image\", r\"E:\\Ben\\TRYazilim\\Figaro-1k\\test\\mask\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafcf089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.6506 - categorical_accuracy: 0.6506 - io_u_1: 0.0000e+00 - loss: 0.6346 - mean_io_u_1: 0.2500 - mse: 0.2220 - precision: 0.6506 - recall: 0.6506\n",
      "Epoch 1: val_loss improved from inf to 0.63374, saving model to model/model_best_best.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 3s/step - accuracy: 0.6508 - categorical_accuracy: 0.6508 - io_u_1: 0.0000e+00 - loss: 0.6344 - mean_io_u_1: 0.2500 - mse: 0.2219 - precision: 0.6508 - recall: 0.6508 - val_accuracy: 0.6799 - val_categorical_accuracy: 0.6799 - val_io_u_1: 0.0000e+00 - val_loss: 0.6337 - val_mean_io_u_1: 0.2500 - val_mse: 0.2208 - val_precision: 0.6799 - val_recall: 0.6799\n",
      "Epoch 2/60\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.6678 - categorical_accuracy: 0.6678 - io_u_1: 0.0000e+00 - loss: 0.6000 - mean_io_u_1: 0.2500 - mse: 0.2067 - precision: 0.6678 - recall: 0.6678\n",
      "Epoch 2: val_loss did not improve from 0.63374\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 3s/step - accuracy: 0.6678 - categorical_accuracy: 0.6678 - io_u_1: 0.0000e+00 - loss: 0.5999 - mean_io_u_1: 0.2500 - mse: 0.2067 - precision: 0.6678 - recall: 0.6678 - val_accuracy: 0.6799 - val_categorical_accuracy: 0.6799 - val_io_u_1: 0.0000e+00 - val_loss: 0.6479 - val_mean_io_u_1: 0.2500 - val_mse: 0.2276 - val_precision: 0.6799 - val_recall: 0.6799\n",
      "Epoch 3/60\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.6953 - categorical_accuracy: 0.6953 - io_u_1: 0.0000e+00 - loss: 0.5697 - mean_io_u_1: 0.2500 - mse: 0.1945 - precision: 0.6953 - recall: 0.6953\n",
      "Epoch 3: val_loss did not improve from 0.63374\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 3s/step - accuracy: 0.6954 - categorical_accuracy: 0.6954 - io_u_1: 0.0000e+00 - loss: 0.5696 - mean_io_u_1: 0.2500 - mse: 0.1945 - precision: 0.6954 - recall: 0.6954 - val_accuracy: 0.3997 - val_categorical_accuracy: 0.3997 - val_io_u_1: 0.0000e+00 - val_loss: 0.7554 - val_mean_io_u_1: 0.2500 - val_mse: 0.2804 - val_precision: 0.3997 - val_recall: 0.3997\n",
      "Epoch 4/60\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7269 - categorical_accuracy: 0.7269 - io_u_1: 0.0000e+00 - loss: 0.5330 - mean_io_u_1: 0.2500 - mse: 0.1806 - precision: 0.7269 - recall: 0.7269\n",
      "Epoch 4: val_loss did not improve from 0.63374\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 2s/step - accuracy: 0.7270 - categorical_accuracy: 0.7270 - io_u_1: 0.0000e+00 - loss: 0.5329 - mean_io_u_1: 0.2500 - mse: 0.1806 - precision: 0.7270 - recall: 0.7270 - val_accuracy: 0.6595 - val_categorical_accuracy: 0.6595 - val_io_u_1: 0.0000e+00 - val_loss: 0.6564 - val_mean_io_u_1: 0.2500 - val_mse: 0.2316 - val_precision: 0.6595 - val_recall: 0.6595\n",
      "Epoch 5/60\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7415 - categorical_accuracy: 0.7415 - io_u_1: 0.0000e+00 - loss: 0.5066 - mean_io_u_1: 0.2500 - mse: 0.1713 - precision: 0.7415 - recall: 0.7415\n",
      "Epoch 5: val_loss improved from 0.63374 to 0.51783, saving model to model/model_best_best.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 2s/step - accuracy: 0.7415 - categorical_accuracy: 0.7415 - io_u_1: 0.0000e+00 - loss: 0.5065 - mean_io_u_1: 0.2500 - mse: 0.1713 - precision: 0.7415 - recall: 0.7415 - val_accuracy: 0.7525 - val_categorical_accuracy: 0.7525 - val_io_u_1: 0.0000e+00 - val_loss: 0.5178 - val_mean_io_u_1: 0.2500 - val_mse: 0.1723 - val_precision: 0.7525 - val_recall: 0.7525\n",
      "Epoch 6/60\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.7460 - categorical_accuracy: 0.7460 - io_u_1: 0.0000e+00 - loss: 0.4971 - mean_io_u_1: 0.2500 - mse: 0.1677 - precision: 0.7460 - recall: 0.7460\n",
      "Epoch 6: val_loss improved from 0.51783 to 0.48594, saving model to model/model_best_best.keras\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 3s/step - accuracy: 0.7461 - categorical_accuracy: 0.7461 - io_u_1: 0.0000e+00 - loss: 0.4970 - mean_io_u_1: 0.2500 - mse: 0.1677 - precision: 0.7461 - recall: 0.7461 - val_accuracy: 0.7595 - val_categorical_accuracy: 0.7595 - val_io_u_1: 0.0000e+00 - val_loss: 0.4859 - val_mean_io_u_1: 0.2500 - val_mse: 0.1624 - val_precision: 0.7595 - val_recall: 0.7595\n",
      "Epoch 7/60\n",
      "\u001b[1m 26/125\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:52\u001b[0m 4s/step - accuracy: 0.7523 - categorical_accuracy: 0.7523 - io_u_1: 0.0000e+00 - loss: 0.4855 - mean_io_u_1: 0.2500 - mse: 0.1633 - precision: 0.7523 - recall: 0.7523"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    trainDataset,\n",
    "    epochs=60,\n",
    "    batch_size=4,\n",
    "    validation_data=testDataset,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aef1b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"modelepoch60.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
